(window.webpackJsonp=window.webpackJsonp||[]).push([[76],{818:function(t,s,a){"use strict";a.r(s);var n=a(15),p=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"网络流"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#网络流"}},[t._v("#")]),t._v(" 网络流")]),t._v(" "),a("h3",{attrs:{id:"最大流的介绍"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#最大流的介绍"}},[t._v("#")]),t._v(" 最大流的介绍")]),t._v(" "),a("p",[t._v("一个水厂需要从河里取水，河边有一个水泵，从河到水厂的中间有很多中转站，中转站、水泵和水厂之间有水管相连。每根水管有一个容量，表示每秒通过该水管的最大流量是多少立方米。现在问，水泵每秒最多能向水厂输送多少立方米的水？")]),t._v(" "),a("p",[t._v("这就是典型的最大流问题。下面给出比较形式化的定义：")]),t._v(" "),a("p",[t._v("给定一个有向图G=(V,E),在E集合里的每条边都由三个元素来描述(u,v,c)，分别表示起点、终点和容量，并指定一个源点S和一个汇点T，一个流必须满足容量限制和流量守恒。")]),t._v(" "),a("p",[t._v("容量限制：f(u,v)<=c(u,v)，形象地理解就是流速不能超过水管的限制。")]),t._v(" "),a("p",[t._v("流量守恒：")]),t._v(" "),a("p",[t._v("形象地理解就是中转站不能把水吞掉了。")]),t._v(" "),a("p",[t._v("上面说的一个可行流，而所谓最大流就是要最大化：其实就是流进汇点的流量。一般我们说一个流的流量都是指流进汇点的流量。")]),t._v(" "),a("h3",{attrs:{id:"最大流最小割定理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#最大流最小割定理"}},[t._v("#")]),t._v(" 最大流最小割定理")]),t._v(" "),a("p",[t._v("割的定义：对于边集E’，若满足去掉该边集中所有边以后，源点和汇点不再连通，那么称边集E’为一个割。割的权定义为对应边集中的所有边的容量之和。最小割指的就是权最小的割。")]),t._v(" "),a("p",[t._v("最大流最小割定理实际上就是说：最大流的流量和最小割的权在数值上相等。")]),t._v(" "),a("h3",{attrs:{id:"残余网络"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#残余网络"}},[t._v("#")]),t._v(" 残余网络")]),t._v(" "),a("p",[t._v("残余网络指的其实是把所有边的权更新为w(u,v)=c(u,v)-f(u,v)。")]),t._v(" "),a("p",[t._v("显然初始时有")]),t._v(" "),a("p",[t._v("w(u,v)=c(u,v)")]),t._v(" "),a("p",[t._v("f(u,v)=0")]),t._v(" "),a("p",[t._v("形象地看，就是说这个水管还能容纳多少流量。一般地，我们只需要注意残余网络就可以求出最大流。为了方便下面都用w(u,v)来表示(u,v)这条边的残余流量。")]),t._v(" "),a("p",[t._v("为了实现下面的增广路算法，这里引入反向弧的概念。所谓反向弧，其实是用以修正前面错误的流的一种修正边。对于原图的每条有向边，我们都给它配对一条反向弧，用以修正该条有向边上错误的流。")]),t._v(" "),a("p",[t._v("假设给定的有向边是(u,v)，那么给它配对的反向弧就是一条(v,u)这样反向的边并且满足以下这个性质：")]),t._v(" "),a("p",[t._v("w(v,u)=f(u,v)")]),t._v(" "),a("p",[t._v("所以有推论w(v,u)+w(u,v)=c(u,v)")]),t._v(" "),a("p",[t._v("因为流的正反向抵消性质(水从u流到v,再从v流回u,其实就是什么都没发生)：")]),t._v(" "),a("ol",[a("li",[a("p",[t._v("如果通过反向弧(v,u)的残余流量增加了流量t，其实相当于f(u,v)=f(u,v)-t，按照上面的性质，其实就是")]),t._v(" "),a("p",[t._v("w(v,u)=w(v,u)-t")]),t._v(" "),a("p",[t._v("w(u,v)=w(u,v)+t")])]),t._v(" "),a("li",[a("p",[t._v("如果通过正向弧(v,u)的残余流量增加了流量t，其实相当于f(u,v)=f(u,v)+t，按照上面的性质，其实就是")]),t._v(" "),a("p",[t._v("w(v,u)=w(v,u)+t")]),t._v(" "),a("p",[t._v("w(u,v)=w(u,v)-t")])])]),t._v(" "),a("p",[t._v("发现了吗？它们是轮换对称的。所以在实际实现中并不需要区分哪条是正向边，哪条是反向弧，假如利用当前边增加了流量t，那么就将当前边的残余流量减去t，再将对应的另一条加上t就可以了。为了方便，我们称正向边和反向弧互为反边，并用op(e)表示e这条边的反边。")]),t._v(" "),a("h3",{attrs:{id:"基于最大流最小割定理求最大流的算法-统称ford-fulkerson算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#基于最大流最小割定理求最大流的算法-统称ford-fulkerson算法"}},[t._v("#")]),t._v(" 基于最大流最小割定理求最大流的算法(统称Ford-Fulkerson算法)")]),t._v(" "),a("p",[t._v("Ford-Fulkerson是求最大流很经典的算法。该算法就是不断在残余网络中寻找增广路并增广，直到找不到增广路为止(也就是说，此时源点和汇点不连通，存在割)。下面给出增广路和增广的含义。")]),t._v(" "),a("p",[t._v("增广路：一条从起始于源点S，终止于汇点T的路径。且必须满足路径上的每条边的残余流量都大于0。")]),t._v(" "),a("p",[t._v("增广：增广是对于一条增广路来说的。令拼接成增广路的有向边组成边集E’。再令 ，然后对于,进行下面操作：")]),t._v(" "),a("p",[t._v("w(e)=w(e)-delta")]),t._v(" "),a("p",[t._v("w(op(e))=w(op(e))+delta")]),t._v(" "),a("p",[t._v("然后流进汇点T的流就加上delta")]),t._v(" "),a("p",[t._v("下面介绍的各种方法其实本质是利用不同的方法来找增广路以达到提高效率的目的。")]),t._v(" "),a("h3",{attrs:{id:"最大容量增广路算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#最大容量增广路算法"}},[t._v("#")]),t._v(" 最大容量增广路算法")]),t._v(" "),a("p",[t._v("这个算法在USACO上有介绍。实际上它每次利用类似dijkstra和prim算法的方式，找出增广时delta值最大的增广路,并对这条增广路进行增广。这个算法一般有不错的效率，但是在实际运用中，往往比不上后面介绍的几种最短增广路算法。")]),t._v(" "),a("h3",{attrs:{id:"最短增广路算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#最短增广路算法"}},[t._v("#")]),t._v(" 最短增广路算法")]),t._v(" "),a("p",[t._v("下面给出的几种算法都是每次找包含边数最少的增广路增广的。运用在信息竞赛上基本都是这几种方法。")]),t._v(" "),a("h3",{attrs:{id:"edmonds-karp"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#edmonds-karp"}},[t._v("#")]),t._v(" Edmonds-Karp")]),t._v(" "),a("p",[t._v("该算法是最简单的最短增广路算法。每次使用广度优先搜索寻找增广路，并对其进行增广。")]),t._v(" "),a("h3",{attrs:{id:"dinic"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dinic"}},[t._v("#")]),t._v(" Dinic")]),t._v(" "),a("p",[t._v("这个算法的效率是相当高的，算法流程如下：")]),t._v(" "),a("ol",[a("li",[a("p",[t._v("每次以源点为起始点bfs，求出每个点的编号d[i]，d[i]表示从源点到i点通过至少几条残余流量大于0的边，能够到达i点。")])]),t._v(" "),a("li",[a("p",[t._v("只有那些满足d[u]+1=d[v]的边(u,v)才被视为存在，然后在这里面不断DFS找增广路并增广(其实随便走都可以)，如果没有增广路了，那么返回步骤1，如果BFS不到汇点，证明算法结束。")])])]),t._v(" "),a("p",[t._v("该算法在第二步DFS有些优化，实现时可以自己想一想。一般不怎么加优化这个算法的效率都很不错了。")]),t._v(" "),a("div",{staticClass:"language-cpp extra-class"},[a("pre",{pre:!0,attrs:{class:"language-cpp"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("bool")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("bfs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("memset")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dis"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("sizeof")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dis"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\tstd"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("::")]),t._v("queue "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" Q"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\tdis"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\tQ"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("push")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" now"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!")]),t._v("Q"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("empty")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\tnow"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("Q"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("front")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\tQ"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("pop")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" i"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("head"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("now"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("next"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t\t\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("c "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!")]),t._v("dis"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t\t\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\t\t\tdis"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("dis"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("now"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t\t\tQ"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("push")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\t\n\t\t\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dis"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("false")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("dfs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" maxf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v("s "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("||")]),t._v(" maxf"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" maxf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" ret"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cur"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("next"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("c "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&&")]),t._v(" dis"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v("dis"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\t\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" f"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("dfs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("std"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("::")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("min")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("maxf"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("ret"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t\te"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("c"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-=")]),t._v("f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t\te"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("c"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v("f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t\tret"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v("f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ret"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v("maxf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t\t\t\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" ret"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\t\t\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("dinic")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("bfs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" i"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v("s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\t\tcur"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("head"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\t\tans"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("dfs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("INF"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),a("h3",{attrs:{id:"sap-推荐使用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sap-推荐使用"}},[t._v("#")]),t._v(" SAP(推荐使用)")]),t._v(" "),a("p",[t._v("SAP同样是效率很高的一个算法，与dinic想比，他只需要两种优化，就能达到dinic在DFS里面加一大堆优化的效率。下面描述该算法的流程：")]),t._v(" "),a("p",[t._v("首先给每个点赋予一个距离标号：d[i]。只有满足d[i]=d[j]+1时，边(i,j)才被视为存在的(暂且称为容许边)。这个标号的意义是到汇点的最短距离。")]),t._v(" "),a("p",[t._v("算法的一开始，我们从汇点进行一次BFS求出距离标号(实际上这一步可以省去)。然后从源点开始递归操作。")]),t._v(" "),a("p",[t._v("对于一个点i的操作：")]),t._v(" "),a("p",[t._v("如果存在容许边(i,j)，当j是汇点时增广并从源点开始递归。否则就令i是j的前驱，并递归j。")]),t._v(" "),a("p",[t._v("如果不存在容许边，则对i重标号，使得d[i]=min{d[j]}+1 (w(i,j)>0)，并回溯。(可以证明重标号操作必然是令d[i]增加的)")]),t._v(" "),a("p",[t._v("当d[s]>=n时算法停止，此时不存在任何一条增广路。")]),t._v(" "),a("p",[t._v("两个重要优化分别是当前弧优化和间隙优化。")]),t._v(" "),a("p",[t._v("当前弧优化：对于一个点i，如果之前递归到这里，并且已经枚举过一些不能增广的边，那么可以证明在这个点重标号之前，之前已无法增广的边不会再次成为容许边。所以记录每个点的出边枚举到那一条，下次直接从这一条开始枚举即可。")]),t._v(" "),a("p",[t._v("间隙优化：假如整个图中，不存在d[i]=k且存在d[i]>k和d[i]⁢k，那么可以退出整个算法了。因为该图已经不存在增广路了。")]),t._v(" "),a("h2",{attrs:{id:"总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结")]),t._v(" "),a("p",[t._v("网络流算法还有另一类算法，它们都是基于预流推进思想的，有兴趣可以看一看。然而用于竞赛中，sap的效率在绝大多数情况下已经足够了。所以如果想要熟练使用一种算法，并在竞赛中快速敲出，推荐sap。另外如果把dinic的优化仔细推敲好了，dinic的效率不会逊色于sap。当然，前提是能在竞赛时熟练敲出。")]),t._v(" "),a("p",[t._v("另外，费用流也是用到这种Ford-Fulkerson算法。")])])}),[],!1,null,null,null);s.default=p.exports}}]);